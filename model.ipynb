{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if GPU + CUDA are available\n",
    "print(torch.cuda.device_count())  # Should print number of GPUs detected\n",
    "print(torch.cuda.current_device())  # Current device index\n",
    "print(torch.cuda.get_device_name(0))  # Name of the GPU device 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.13.1 in e:\\speecrecognition\\venv\\lib\\site-packages (2.13.1)\n",
      "Requirement already satisfied: transformers==4.35.0 in e:\\speecrecognition\\venv\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: torchaudio==2.1.0 in e:\\speecrecognition\\venv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: librosa==0.10.0 in e:\\speecrecognition\\venv\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: jiwer in e:\\speecrecognition\\venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: evaluate in e:\\speecrecognition\\venv\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: soundfile in e:\\speecrecognition\\venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (0.3.6)\n",
      "Requirement already satisfied: pandas in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets==2.13.1) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (0.17.3)\n",
      "Requirement already satisfied: packaging in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from datasets==2.13.1) (6.0.2)\n",
      "Requirement already satisfied: filelock in e:\\speecrecognition\\venv\\lib\\site-packages (from transformers==4.35.0) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\speecrecognition\\venv\\lib\\site-packages (from transformers==4.35.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in e:\\speecrecognition\\venv\\lib\\site-packages (from transformers==4.35.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from transformers==4.35.0) (0.5.3)\n",
      "Requirement already satisfied: torch==2.1.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from torchaudio==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (5.2.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (0.61.2)\n",
      "Requirement already satisfied: pooch>=1.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (4.13.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from librosa==0.10.0) (1.1.0)\n",
      "Requirement already satisfied: sympy in e:\\speecrecognition\\venv\\lib\\site-packages (from torch==2.1.0->torchaudio==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in e:\\speecrecognition\\venv\\lib\\site-packages (from torch==2.1.0->torchaudio==2.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\speecrecognition\\venv\\lib\\site-packages (from torch==2.1.0->torchaudio==2.1.0) (3.1.6)\n",
      "Requirement already satisfied: click>=8.1.8 in e:\\speecrecognition\\venv\\lib\\site-packages (from jiwer) (8.2.0)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in e:\\speecrecognition\\venv\\lib\\site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: cffi>=1.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in e:\\speecrecognition\\venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: colorama in e:\\speecrecognition\\venv\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from aiohttp->datasets==2.13.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\speecrecognition\\venv\\lib\\site-packages (from aiohttp->datasets==2.13.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from aiohttp->datasets==2.13.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from aiohttp->datasets==2.13.1) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\speecrecognition\\venv\\lib\\site-packages (from aiohttp->datasets==2.13.1) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from aiohttp->datasets==2.13.1) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from aiohttp->datasets==2.13.1) (1.20.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in e:\\speecrecognition\\venv\\lib\\site-packages (from numba>=0.51.0->librosa==0.10.0) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from pooch>=1.0->librosa==0.10.0) (4.3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\speecrecognition\\venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.13.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\speecrecognition\\venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.13.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.13.1) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\speecrecognition\\venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.13.1) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa==0.10.0) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\speecrecognition\\venv\\lib\\site-packages (from pandas->datasets==2.13.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from pandas->datasets==2.13.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\speecrecognition\\venv\\lib\\site-packages (from pandas->datasets==2.13.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\speecrecognition\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.13.1) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from jinja2->torch==2.1.0->torchaudio==2.1.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from sympy->torch==2.1.0->torchaudio==2.1.0) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\True\\AppData\\Local\\Temp\\ipykernel_15688\\2395030648.py\", line 7, in <module>\n",
      "    import torch\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\torch\\__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\torch\\functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"e:\\speecRecognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.13.1 transformers==4.35.0 torchaudio==2.1.0 librosa==0.10.0 jiwer evaluate soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in e:\\speecrecognition\\venv\\lib\\site-packages (0.28.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in e:\\speecrecognition\\venv\\lib\\site-packages (from accelerate) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in e:\\speecrecognition\\venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in e:\\speecrecognition\\venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from accelerate) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in e:\\speecrecognition\\venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in e:\\speecrecognition\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in e:\\speecrecognition\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\speecrecognition\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: sympy in e:\\speecrecognition\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in e:\\speecrecognition\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\speecrecognition\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: colorama in e:\\speecrecognition\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\speecrecognition\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\speecrecognition\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\speecrecognition\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\speecrecognition\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\speecrecognition\\venv\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.28.0\n",
      "    Uninstalling accelerate-0.28.0:\n",
      "      Successfully uninstalled accelerate-0.28.0\n",
      "Successfully installed accelerate-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\speecRecognition\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import re\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/602 [00:00<?, ? examples/s]e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from C:\\Users\\True\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--wer\\85bee9e4216a78bb09b2d0d500f6af5c23da58f9210e661add540f5df6630fcd (last modified on Mon May  5 12:58:53 2025) since it couldn't be found locally at evaluate-metric--wer, or remotely on the Hugging Face Hub.\n",
      "C:\\Users\\True\\AppData\\Local\\Temp\\ipykernel_31260\\2916721480.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3010' max='3010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3010/3010 2:34:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.490700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.966600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.559500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.620900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.373600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.104900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3010, training_loss=0.6875608202626935, metrics={'train_runtime': 9283.1176, 'train_samples_per_second': 0.648, 'train_steps_per_second': 0.324, 'total_flos': 3.416416177825872e+17, 'train_loss': 0.6875608202626935, 'epoch': 10.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Loading the csv file that maps audios to their transcriptions.\n",
    "csv_file = \"mapping.csv\"  # path to mapping.csv\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "audio_dir = os.path.dirname(csv_file)\n",
    "if \"path\" in df.columns:\n",
    "    df[\"audio_path\"] = df[\"path\"]\n",
    "elif \"filename\" in df.columns:\n",
    "    df[\"audio_path\"] = df[\"filename\"].apply(lambda x: os.path.join(audio_dir, x))\n",
    "elif \"file\" in df.columns:\n",
    "    df[\"audio_path\"] = df[\"file\"].apply(lambda x: os.path.join(audio_dir, x))\n",
    "elif \"audio\" in df.columns:\n",
    "    df[\"audio_path\"] = df[\"audio\"]\n",
    "else:\n",
    "    raise ValueError(\"No audio file path column found in CSV.\")\n",
    "\n",
    "\n",
    "if \"text\" in df.columns:\n",
    "    pass\n",
    "elif \"transcript\" in df.columns:\n",
    "    df = df.rename(columns={\"transcript\": \"text\"})\n",
    "elif \"sentence\" in df.columns:\n",
    "    df = df.rename(columns={\"sentence\": \"text\"})\n",
    "elif \"transcription\" in df.columns:\n",
    "    df = df.rename(columns={\"transcription\": \"text\"})\n",
    "else:\n",
    "    \n",
    "    df = df.rename(columns={df.columns[-1]: \"text\"})\n",
    "\n",
    "\n",
    "# Removing any rows with missing values\n",
    "df = df.dropna(subset=[\"audio_path\",\"text\"]).reset_index(drop=True)\n",
    "\n",
    "# Create a HuggingFace Dataset from the pandas DataFrame so to fit wav2vec\n",
    "dataset = Dataset.from_pandas(df[[\"audio_path\", \"text\"]])\n",
    "\n",
    "dataset = dataset.rename_column(\"audio_path\", \"audio\")\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "\n",
    "# Split into training and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# a text cleaning function\n",
    "chars_to_ignore_regex = r\"[\\,\\?\\.\\!\\-\\;\\:\\\"]\"\n",
    "def preprocess_text(batch):\n",
    "    text = batch[\"text\"]\n",
    "    text = text.lower()\n",
    "    text = re.sub(chars_to_ignore_regex, \"\", text)\n",
    "    text = text.strip()\n",
    "    batch[\"text\"] = text\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_text)\n",
    "eval_dataset = eval_dataset.map(preprocess_text)\n",
    "\n",
    "# Load pre-trained processor and model for Kazakh (Trained in voice of young Kazakhs)\n",
    "model_name = \"aismlv/wav2vec2-large-xlsr-kazakh\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    ")\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# bactching audios and adding paddings to insure equal length + CTC loss -\n",
    "# function to address timing issues in audio.\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "# Preprocess dataset (feature extraction + adding noise)\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    speech = audio[\"array\"]\n",
    "    # Data augmentation: add random noise\n",
    "    if random.random() < 0.3:\n",
    "        noise = np.random.randn(len(speech))\n",
    "        speech = speech + 0.005 * noise\n",
    "        speech = np.clip(speech, -1, 1)\n",
    "    batch[\"input_values\"] = processor(speech, sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    #Converting the text transcriptions into list of token IDs so to knwo the prediction to the correct answer)\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=[\"audio\", \"text\"])\n",
    "eval_dataset = eval_dataset.map(prepare_dataset, remove_columns=[\"audio\", \"text\"])\n",
    "\n",
    "# Load WER metric for evaluation (WER = Word Error Rate)\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# Define TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2_kazakh_child\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,  # Helps stabilize early training\n",
    "    weight_decay=0.01,  # Slight increase for regularization\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/67 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/67 [00:00<?, ? examples/s]e:\\speecRecognition\\venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from C:\\Users\\True\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--wer\\85bee9e4216a78bb09b2d0d500f6af5c23da58f9210e661add540f5df6630fcd (last modified on Mon May  5 12:58:53 2025) since it couldn't be found locally at evaluate-metric--wer, or remotely on the Hugging Face Hub.\n",
      "C:\\Users\\True\\AppData\\Local\\Temp\\ipykernel_19336\\863657238.py:143: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.5918092131614685, 'eval_model_preparation_time': 0.003, 'eval_wer': 0.2247191011235955, 'eval_runtime': 2.6121, 'eval_samples_per_second': 25.649, 'eval_steps_per_second': 13.016}\n"
     ]
    }
   ],
   "source": [
    "# Inference & Evaluation in a Single Jupyter Cell\n",
    "\n",
    "# 1) Imports\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Union, Optional\n",
    "import evaluate\n",
    "\n",
    "# 2) Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 3) Load CSV & rebuild eval_dataset exactly as during training\n",
    "csv_file = \"mapping.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create full audio_path column\n",
    "audio_dir = os.path.dirname(csv_file)\n",
    "if \"path\" in df.columns:\n",
    "    df[\"audio_path\"] = df[\"path\"]\n",
    "elif \"filename\" in df.columns:\n",
    "    df[\"audio_path\"] = df[\"filename\"].apply(lambda x: os.path.join(audio_dir, x))\n",
    "elif \"file\" in df.columns:\n",
    "    df[\"audio_path\"] = df[\"file\"].apply(lambda x: os.path.join(audio_dir, x))\n",
    "elif \"audio\" in df.columns:\n",
    "    df[\"audio_path\"] = df[\"audio\"]\n",
    "else:\n",
    "    raise ValueError(\"No audio file path column found in CSV.\")\n",
    "\n",
    "# Rename transcription to 'text'\n",
    "if \"text\" not in df.columns:\n",
    "    if \"transcript\" in df.columns:\n",
    "        df = df.rename(columns={\"transcript\": \"text\"})\n",
    "    elif \"sentence\" in df.columns:\n",
    "        df = df.rename(columns={\"sentence\": \"text\"})\n",
    "    elif \"transcription\" in df.columns:\n",
    "        df = df.rename(columns={\"transcription\": \"text\"})\n",
    "    else:\n",
    "        df = df.rename(columns={df.columns[-1]: \"text\"})\n",
    "\n",
    "# Drop missing\n",
    "df = df.dropna(subset=[\"audio_path\", \"text\"]).reset_index(drop=True)\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"audio_path\", \"text\"]])\n",
    "dataset = dataset.rename_column(\"audio_path\", \"audio\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# Text preprocessing\n",
    "chars_to_ignore_regex = r\"[\\,\\?\\.\\!\\-\\;\\:\\\"]\"\n",
    "def preprocess_text(batch):\n",
    "    text = batch[\"text\"].lower()\n",
    "    text = re.sub(chars_to_ignore_regex, \"\", text).strip()\n",
    "    batch[\"text\"] = text\n",
    "    return batch\n",
    "\n",
    "eval_dataset = eval_dataset.map(preprocess_text)\n",
    "\n",
    "# 4) Load processor & model\n",
    "ckpt_dir         = \"./wav2vec2_kazakh_child/checkpoint-3010\"  # <-- your checkpoint folder\n",
    "base_model_name  = \"aismlv/wav2vec2-large-xlsr-kazakh\"\n",
    "\n",
    "# 4a) Processor must come from the original pretrained model\n",
    "processor = Wav2Vec2Processor.from_pretrained(base_model_name)\n",
    "\n",
    "# 4b) Model weights come from the checkpoint\n",
    "model = Wav2Vec2ForCTC.from_pretrained(ckpt_dir).to(device)\n",
    "\n",
    "# 5) Prepare eval_dataset (input_values + labels)\n",
    "def prepare_dataset(batch):\n",
    "    speech = batch[\"audio\"][\"array\"]\n",
    "    batch[\"input_values\"] = processor(speech, sampling_rate=16000).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "eval_dataset = eval_dataset.map(prepare_dataset, remove_columns=[\"audio\", \"text\"])\n",
    "\n",
    "# 6) Data collator (same as training)\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "# 7) WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# 8) Run evaluation on the validation set\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2_kazakh_eval\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Validation metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted : \n",
      "Actual    : \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Specify the path to your test WAV\n",
    "wav_path = \"audio/some_test.wav\"  # <-- replace with your file\n",
    "\n",
    "# 2) Run inference through the finetuned model\n",
    "speech, sr = sf.read(wav_path)\n",
    "if sr != 16000:\n",
    "    import torchaudio\n",
    "    speech = torchaudio.functional.resample(torch.tensor(speech), sr, 16000).numpy()\n",
    "\n",
    "inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs).logits\n",
    "\n",
    "pred_ids      = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "pred_text     = processor.batch_decode([pred_ids])[0]\n",
    "\n",
    "# 3) Look up the actual transcription in your DataFrame\n",
    "#    We assume df[\"audio_path\"] holds the same relative path string as wav_path.\n",
    "row = df[df[\"audio_path\"] == wav_path]\n",
    "if len(row) == 1:\n",
    "    actual_text = row[\"text\"].values[0]\n",
    "else:\n",
    "    actual_text = \"<not found in mapping.csv>\"\n",
    "\n",
    "# 4) Print both\n",
    "print(f\"Predicted : {pred_text}\")\n",
    "print(f\"Actual    : {actual_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
